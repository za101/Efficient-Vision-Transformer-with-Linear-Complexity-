{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFFXOJOGIdBA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SoftTargetCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    The native CE loss with soft target\n",
        "    input: x is output of model, target is ground truth\n",
        "    return: loss\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(SoftTargetCrossEntropy, self).__init__()\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        N_rep = x.shape[0]\n",
        "        N = target.shape[0]\n",
        "        if not N == N_rep:\n",
        "            target = target.repeat(N_rep // N, 1)\n",
        "        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class TokenLabelGTCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    Token labeling dense loss with ground gruth, see more from token labeling\n",
        "    input: x is output of model, target is ground truth\n",
        "    return: loss\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 dense_weight=1.0,\n",
        "                 cls_weight=1.0,\n",
        "                 mixup_active=True,\n",
        "                 smoothing=0.1,\n",
        "                 classes=1000):\n",
        "        super(TokenLabelGTCrossEntropy, self).__init__()\n",
        "\n",
        "        self.CE = SoftTargetCrossEntropy()\n",
        "\n",
        "        self.dense_weight = dense_weight\n",
        "        self.smoothing = smoothing\n",
        "        self.mixup_active = mixup_active\n",
        "        self.classes = classes\n",
        "        self.cls_weight = cls_weight\n",
        "        assert dense_weight + cls_weight > 0\n",
        "\n",
        "    def forward(self, x, target):\n",
        "\n",
        "        output, aux_output, bb = x\n",
        "        bbx1, bby1, bbx2, bby2 = bb\n",
        "\n",
        "        B, N, C = aux_output.shape\n",
        "        if len(target.shape) == 2:\n",
        "            target_cls = target\n",
        "            target_aux = target.repeat(1, N).reshape(B * N, C)\n",
        "        else:\n",
        "            ground_truth = target[:, :, 0]\n",
        "            target_cls = target[:, :, 1]\n",
        "            ratio = (0.9 - 0.4 *\n",
        "                     (ground_truth.max(-1)[1] == target_cls.max(-1)[1])\n",
        "                     ).unsqueeze(-1)\n",
        "            target_cls = target_cls * ratio + ground_truth * (1 - ratio)\n",
        "            target_aux = target[:, :, 2:]\n",
        "            target_aux = target_aux.transpose(1, 2).reshape(-1, C)\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / N)\n",
        "        if lam < 1:\n",
        "            target_cls = lam * target_cls + (1 - lam) * target_cls.flip(0)\n",
        "\n",
        "        aux_output = aux_output.reshape(-1, C)\n",
        "\n",
        "        loss_cls = self.CE(output, target_cls)\n",
        "        loss_aux = self.CE(aux_output, target_aux)\n",
        "\n",
        "        return self.cls_weight * loss_cls + self.dense_weight * loss_aux\n",
        "\n",
        "\n",
        "class TokenLabelSoftTargetCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    Token labeling dense loss with soft target, see more from token labeling\n",
        "    input: x is output of model, target is ground truth\n",
        "    return: loss\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(TokenLabelSoftTargetCrossEntropy, self).__init__()\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        N_rep = x.shape[0]\n",
        "        N = target.shape[0]\n",
        "        if not N == N_rep:\n",
        "            target = target.repeat(N_rep // N, 1)\n",
        "        if len(target.shape) == 3 and target.shape[-1] == 2:\n",
        "            target = target[:, :, 1]\n",
        "        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "class TokenLabelCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    Token labeling loss without ground truth\n",
        "    input: x is output of model, target is ground truth\n",
        "    return: loss\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 dense_weight=1.0,\n",
        "                 cls_weight=1.0,\n",
        "                 mixup_active=True,\n",
        "                 classes=1000):\n",
        "        \"\"\"\n",
        "        Constructor Token labeling loss.\n",
        "        \"\"\"\n",
        "        super(TokenLabelCrossEntropy, self).__init__()\n",
        "\n",
        "        self.CE = SoftTargetCrossEntropy()\n",
        "\n",
        "        self.dense_weight = dense_weight\n",
        "        self.mixup_active = mixup_active\n",
        "        self.classes = classes\n",
        "        self.cls_weight = cls_weight\n",
        "        assert dense_weight + cls_weight > 0\n",
        "\n",
        "    def forward(self, x, target):\n",
        "\n",
        "        output, aux_output, bb = x\n",
        "        bbx1, bby1, bbx2, bby2 = bb\n",
        "\n",
        "        B, N, C = aux_output.shape\n",
        "        if len(target.shape) == 2:\n",
        "            target_cls = target\n",
        "            target_aux = target.repeat(1, N).reshape(B * N, C)\n",
        "        else:\n",
        "            target_cls = target[:, :, 1]\n",
        "            target_aux = target[:, :, 2:]\n",
        "            target_aux = target_aux.transpose(1, 2).reshape(-1, C)\n",
        "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / N)\n",
        "        if lam < 1:\n",
        "            target_cls = lam * target_cls + (1 - lam) * target_cls.flip(0)\n",
        "\n",
        "        aux_output = aux_output.reshape(-1, C)\n",
        "        loss_cls = self.CE(output, target_cls)\n",
        "        loss_aux = self.CE(aux_output, target_aux)\n",
        "        return self.cls_weight * loss_cls + self.dense_weight * loss_aux"
      ]
    }
  ]
}